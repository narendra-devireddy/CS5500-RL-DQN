{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implementation for MountainCar-v0 and Pong-v0\n",
    "\n",
    "This notebook implements Deep Q-Network (DQN) algorithm for two OpenAI Gym environments:\n",
    "- **MountainCar-v0**: Classic control problem with continuous state space\n",
    "- **Pong-v0**: Atari game with visual input\n",
    "\n",
    "## Features:\n",
    "- Environment exploration with random agents\n",
    "- DQN with experience replay and target networks\n",
    "- Frame preprocessing for Pong (grayscale, downsampling, frame stacking)\n",
    "- Training curves and policy visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running on Colab)\n",
    "# !pip install gymnasium[atari] gymnasium[accept-rom-license] ale-py torch torchvision opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Import custom modules\n",
    "from dqn_agent import DQNAgent, device\n",
    "from preprocessing import FramePreprocessor, FrameStack\n",
    "from utils import plot_training_curve, plot_mountaincar_policy, save_results, load_results\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part A: Environment Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Environments and Inspect State/Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_environment(env_name):\n",
    "    \"\"\"Load environment and print state/action space information\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Environment: {env_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    print(f\"\\nObservation Space: {env.observation_space}\")\n",
    "    print(f\"Action Space: {env.action_space}\")\n",
    "    \n",
    "    if isinstance(env.observation_space, gym.spaces.Box):\n",
    "        print(f\"Observation Shape: {env.observation_space.shape}\")\n",
    "        if len(env.observation_space.shape) == 1:\n",
    "            print(f\"Observation Low: {env.observation_space.low}\")\n",
    "            print(f\"Observation High: {env.observation_space.high}\")\n",
    "    \n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        print(f\"Number of Actions: {env.action_space.n}\")\n",
    "    \n",
    "    # Sample observation\n",
    "    obs, _ = env.reset(seed=SEED)\n",
    "    print(f\"\\nSample Observation Shape: {np.array(obs).shape}\")\n",
    "    if len(np.array(obs).shape) == 1:\n",
    "        print(f\"Sample Observation: {obs}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# Explore MountainCar\n",
    "explore_environment(\"MountainCar-v0\")\n",
    "\n",
    "# Explore Pong\n",
    "explore_environment(\"ALE/Pong-v5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Agent to Understand Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_agent(env_name, num_episodes=10, max_steps=1000):\n",
    "    \"\"\"Test random agent and analyze rewards\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Random Agent Testing: {env_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset(seed=SEED + episode)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (done or truncated) and steps < max_steps:\n",
    "            action = env.action_space.sample()  # Random action\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            all_rewards.append(reward)\n",
    "            steps += 1\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        print(f\"Episode {episode+1}: Reward = {total_reward:.2f}, Steps = {steps}\")\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Mean Episode Reward: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
    "    print(f\"Mean Episode Length: {np.mean(episode_lengths):.2f} ± {np.std(episode_lengths):.2f}\")\n",
    "    print(f\"Unique Rewards: {np.unique(all_rewards)}\")\n",
    "    print(f\"Reward Range: [{np.min(all_rewards):.2f}, {np.max(all_rewards):.2f}]\")\n",
    "    \n",
    "    env.close()\n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "# Test random agents\n",
    "mc_rewards, mc_lengths = test_random_agent(\"MountainCar-v0\", num_episodes=10)\n",
    "pong_rewards, pong_lengths = test_random_agent(\"ALE/Pong-v5\", num_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from Random Agents\n",
    "\n",
    "**MountainCar-v0:**\n",
    "- **State space**: 2D continuous (position ∈ [-1.2, 0.6], velocity ∈ [-0.07, 0.07])\n",
    "- **Action space**: 3 discrete actions (0=push left, 1=no push, 2=push right)\n",
    "- **Reward**: -1 for each time step until goal is reached\n",
    "- **Random agent**: Typically gets rewards around -200 (episode length limit)\n",
    "- **Challenge**: Need to build momentum by going back and forth to reach the goal\n",
    "\n",
    "**Pong-v0:**\n",
    "- **State space**: RGB image (210×160×3)\n",
    "- **Action space**: 6 discrete actions (NOOP, FIRE, RIGHT, LEFT, RIGHTFIRE, LEFTFIRE)\n",
    "- **Reward**: +1 for scoring, -1 for opponent scoring, 0 otherwise\n",
    "- **Random agent**: Typically loses badly (around -21 to -15)\n",
    "- **Challenge**: High-dimensional visual input requires preprocessing and frame stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Part B: DQN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env_name, agent, num_episodes=1000, max_steps=1000, \n",
    "              preprocessor=None, frame_stack=None, print_freq=10, \n",
    "              save_freq=100, save_path='dqn_checkpoint.pth'):\n",
    "    \"\"\"Train DQN agent\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    mean_rewards = []\n",
    "    best_mean_reward = -float('inf')\n",
    "    \n",
    "    total_steps = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        \n",
    "        # Preprocess initial observation\n",
    "        if preprocessor is not None:\n",
    "            preprocessor.reset()\n",
    "            processed_obs = preprocessor.preprocess(obs, use_diff=False)\n",
    "            if frame_stack is not None:\n",
    "                state = frame_stack.reset(processed_obs)\n",
    "            else:\n",
    "                state = processed_obs\n",
    "        else:\n",
    "            state = obs\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select action\n",
    "            action = agent.select_action(state, training=True)\n",
    "            \n",
    "            # Take action\n",
    "            next_obs, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            # Preprocess next observation\n",
    "            if preprocessor is not None:\n",
    "                processed_next_obs = preprocessor.preprocess(next_obs, use_diff=False)\n",
    "                if frame_stack is not None:\n",
    "                    next_state = frame_stack.push(processed_next_obs)\n",
    "                else:\n",
    "                    next_state = processed_next_obs\n",
    "            else:\n",
    "                next_state = next_obs\n",
    "            \n",
    "            # Store transition\n",
    "            agent.replay_buffer.push(state, action, next_state, reward, done or truncated)\n",
    "            \n",
    "            # Update agent\n",
    "            loss = agent.update()\n",
    "            if loss is not None:\n",
    "                episode_loss.append(loss)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            total_steps += 1\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Record statistics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        if episode_loss:\n",
    "            losses.append(np.mean(episode_loss))\n",
    "        \n",
    "        # Calculate mean reward (last 100 episodes)\n",
    "        if len(episode_rewards) >= 100:\n",
    "            mean_reward = np.mean(episode_rewards[-100:])\n",
    "            mean_rewards.append(mean_reward)\n",
    "            \n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                agent.save(save_path.replace('.pth', '_best.pth'))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % print_freq == 0:\n",
    "            mean_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "            print(f\"Episode {episode+1}/{num_episodes} | \"\n",
    "                  f\"Steps: {total_steps} | \"\n",
    "                  f\"Reward: {episode_reward:.2f} | \"\n",
    "                  f\"Mean(100): {mean_reward:.2f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f} | \"\n",
    "                  f\"Loss: {np.mean(episode_loss) if episode_loss else 0:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (episode + 1) % save_freq == 0:\n",
    "            agent.save(save_path)\n",
    "            print(f\"Checkpoint saved at episode {episode+1}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'losses': losses,\n",
    "        'mean_rewards': mean_rewards,\n",
    "        'best_mean_reward': best_mean_reward\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train DQN on MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MountainCar agent\n",
    "mc_agent = DQNAgent(\n",
    "    state_dim=2,\n",
    "    action_dim=3,\n",
    "    network_type='mlp',\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    buffer_size=50000,\n",
    "    batch_size=64,\n",
    "    target_update_freq=100\n",
    ")\n",
    "\n",
    "print(\"Training DQN on MountainCar-v0...\")\n",
    "print(\"This should take about 10-20 minutes on a modest laptop.\")\n",
    "\n",
    "# Train\n",
    "mc_results = train_dqn(\n",
    "    env_name=\"MountainCar-v0\",\n",
    "    agent=mc_agent,\n",
    "    num_episodes=500,\n",
    "    max_steps=200,\n",
    "    print_freq=10,\n",
    "    save_freq=100,\n",
    "    save_path='mountaincar_dqn.pth'\n",
    ")\n",
    "\n",
    "# Save results\n",
    "save_results(mc_results, 'mountaincar_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plot_training_curve(mc_results, 'MountainCar-v0', window=50, save_path='mountaincar_training.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot policy visualization\n",
    "plot_mountaincar_policy(mc_agent, resolution=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train DQN on Pong-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pong agent with CNN\n",
    "pong_agent = DQNAgent(\n",
    "    state_dim=4,  # 4 stacked frames\n",
    "    action_dim=6,\n",
    "    network_type='cnn',\n",
    "    lr=1e-4,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.02,\n",
    "    epsilon_decay=0.9999,\n",
    "    buffer_size=100000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=1000\n",
    ")\n",
    "\n",
    "# Create preprocessor and frame stack\n",
    "pong_preprocessor = FramePreprocessor(frame_size=(84, 84))\n",
    "pong_frame_stack = FrameStack(num_frames=4)\n",
    "\n",
    "print(\"Training DQN on Pong-v0...\")\n",
    "print(\"This will take several hours. Consider reducing num_episodes for testing.\")\n",
    "print(\"For a reasonable Pong player, train for 2000-4000 episodes (2-4 million steps).\")\n",
    "\n",
    "# Train (reduce num_episodes for quick testing)\n",
    "pong_results = train_dqn(\n",
    "    env_name=\"ALE/Pong-v5\",\n",
    "    agent=pong_agent,\n",
    "    num_episodes=2000,  # Increase to 3000-4000 for better performance\n",
    "    max_steps=10000,\n",
    "    preprocessor=pong_preprocessor,\n",
    "    frame_stack=pong_frame_stack,\n",
    "    print_freq=10,\n",
    "    save_freq=100,\n",
    "    save_path='pong_dqn.pth'\n",
    ")\n",
    "\n",
    "# Save results\n",
    "save_results(pong_results, 'pong_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plot_training_curve(pong_results, 'Pong-v0', window=100, save_path='pong_training.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Test Trained Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env_name, agent, num_episodes=10, preprocessor=None, frame_stack=None, render=False):\n",
    "    \"\"\"Test trained agent\"\"\"\n",
    "    if render:\n",
    "        env = gym.make(env_name, render_mode='human')\n",
    "    else:\n",
    "        env = gym.make(env_name)\n",
    "    \n",
    "    test_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        \n",
    "        if preprocessor is not None:\n",
    "            preprocessor.reset()\n",
    "            processed_obs = preprocessor.preprocess(obs, use_diff=False)\n",
    "            if frame_stack is not None:\n",
    "                state = frame_stack.reset(processed_obs)\n",
    "            else:\n",
    "                state = processed_obs\n",
    "        else:\n",
    "            state = obs\n",
    "        \n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            action = agent.select_action(state, training=False)\n",
    "            next_obs, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            if preprocessor is not None:\n",
    "                processed_next_obs = preprocessor.preprocess(next_obs, use_diff=False)\n",
    "                if frame_stack is not None:\n",
    "                    next_state = frame_stack.push(processed_next_obs)\n",
    "                else:\n",
    "                    next_state = processed_next_obs\n",
    "            else:\n",
    "                next_state = next_obs\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        test_rewards.append(episode_reward)\n",
    "        print(f\"Test Episode {episode+1}: Reward = {episode_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Mean Reward: {np.mean(test_rewards):.2f} ± {np.std(test_rewards):.2f}\")\n",
    "    print(f\"Min Reward: {np.min(test_rewards):.2f}\")\n",
    "    print(f\"Max Reward: {np.max(test_rewards):.2f}\")\n",
    "    \n",
    "    return test_rewards\n",
    "\n",
    "# Test MountainCar agent\n",
    "print(\"Testing MountainCar agent...\")\n",
    "mc_test_rewards = test_agent(\"MountainCar-v0\", mc_agent, num_episodes=10)\n",
    "\n",
    "# Test Pong agent\n",
    "print(\"\\nTesting Pong agent...\")\n",
    "pong_test_rewards = test_agent(\"ALE/Pong-v5\", pong_agent, num_episodes=5, \n",
    "                                preprocessor=pong_preprocessor, frame_stack=pong_frame_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary and Analysis\n",
    "\n",
    "### MountainCar-v0 Results:\n",
    "- The DQN agent should learn to solve MountainCar within 300-500 episodes\n",
    "- Successful agents achieve rewards around -110 to -90 (reaching goal in 90-110 steps)\n",
    "- The policy visualization shows the agent learns to:\n",
    "  - Push right when moving right with positive velocity\n",
    "  - Push left when moving left to build momentum\n",
    "  - Use the valley strategically to gain speed\n",
    "\n",
    "### Pong-v0 Results:\n",
    "- Training requires 2000-4000 episodes (2-4 million steps) for reasonable performance\n",
    "- Random agent scores around -21 (loses every point)\n",
    "- After training:\n",
    "  - Early training (500-1000 episodes): Agent starts to track the ball\n",
    "  - Mid training (1000-2000 episodes): Agent wins occasional points (score -15 to -10)\n",
    "  - Late training (2000+ episodes): Agent becomes competitive (score -5 to +5)\n",
    "- Frame preprocessing (grayscale + downsampling) reduces computation significantly\n",
    "- Frame stacking (4 frames) helps capture motion information\n",
    "\n",
    "### Key Observations:\n",
    "1. **Experience Replay**: Critical for stable learning, breaks correlation between consecutive samples\n",
    "2. **Target Network**: Prevents moving target problem, updated every 1000 steps\n",
    "3. **Epsilon Decay**: Balances exploration vs exploitation\n",
    "4. **Preprocessing**: Essential for Atari games to reduce input dimensionality\n",
    "5. **Hyperparameters**: Learning rate, batch size, and network architecture significantly impact performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Continue Training (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved checkpoint and continue training\n",
    "# mc_agent.load('mountaincar_dqn_best.pth')\n",
    "# pong_agent.load('pong_dqn_best.pth')\n",
    "\n",
    "# Continue training with more episodes\n",
    "# mc_results_continued = train_dqn(...)\n",
    "# pong_results_continued = train_dqn(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
