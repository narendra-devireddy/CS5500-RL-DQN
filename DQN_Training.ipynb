{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6WIPP9_KbH0"
      },
      "source": [
        "# DQN Implementation for MountainCar-v0 and Pong-v0\n",
        "\n",
        "This notebook implements Deep Q-Network (DQN) algorithm for two OpenAI Gym environments:\n",
        "- **MountainCar-v0**: Classic control problem with continuous state space\n",
        "- **Pong-v0**: Atari game with visual input\n",
        "\n",
        "## Features:\n",
        "- Environment exploration with random agents\n",
        "- DQN with experience replay and target networks\n",
        "- Frame preprocessing for Pong (grayscale, downsampling, frame stacking)\n",
        "- Training curves and policy visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5fpweOWKbH2"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/narendra-devireddy/CS5500-RL-DQN.git"
      ],
      "metadata": {
        "id": "M5zpJq1FLQU6",
        "outputId": "789f5294-6f61-4710-d879-cb818eb65954",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CS5500-RL-DQN'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 15 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (15/15), 175.16 KiB | 1.56 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to the directory\n",
        "%cd CS5500-RL-DQN"
      ],
      "metadata": {
        "id": "2jL4YkOfL4PW",
        "outputId": "40ebf941-bfa7-4e8a-9293-741e989726ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CS5500-RL-DQN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1ZoQlJXLKbH2",
        "outputId": "6017ef95-5ec5-4edb-a601-07da888fa183",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.12/dist-packages (0.11.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[atari]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "\u001b[33mWARNING: gymnasium 1.2.1 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (uncomment if running on Colab)\n",
        "!pip install gymnasium[atari] gymnasium[accept-rom-license] ale-py torch torchvision opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ale_py.roms as roms\n",
        "print(\"✅ ROMs installed!\")"
      ],
      "metadata": {
        "id": "2K7cFYN3MMBD",
        "outputId": "fb5d7b3e-38d9-4484-8564-74a7574354d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ROMs installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Import and test\n",
        "import gymnasium as gym\n",
        "env = gym.make(\"ALE/Pong-v5\")\n",
        "print(\"✅ Success!\")"
      ],
      "metadata": {
        "id": "SMPAKuU4MfzX",
        "outputId": "4d10daed-d30c-44ac-9d80-f8648bacde2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KLTQEiybKbH3",
        "outputId": "6f3b6ff2-e439-4177-e751-c03f38b9cd98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Import custom modules\n",
        "from dqn_agent import DQNAgent, device\n",
        "from preprocessing import FramePreprocessor, FrameStack\n",
        "from utils import plot_training_curve, plot_mountaincar_policy, save_results, load_results\n",
        "\n",
        "# Set random seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlwsl1m8KbH3"
      },
      "source": [
        "## 2. Part A: Environment Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwTnvTgGKbH3"
      },
      "source": [
        "### 2.1 Load Environments and Inspect State/Action Spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qf3meoZuKbH3",
        "outputId": "5c01c21d-bf99-4669-b8d5-0d0769149bee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Environment: MountainCar-v0\n",
            "============================================================\n",
            "\n",
            "Observation Space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
            "Action Space: Discrete(3)\n",
            "Observation Shape: (2,)\n",
            "Observation Low: [-1.2  -0.07]\n",
            "Observation High: [0.6  0.07]\n",
            "Number of Actions: 3\n",
            "\n",
            "Sample Observation Shape: (2,)\n",
            "Sample Observation: [-0.4452088  0.       ]\n",
            "\n",
            "============================================================\n",
            "Environment: ALE/Pong-v5\n",
            "============================================================\n",
            "\n",
            "Observation Space: Box(0, 255, (210, 160, 3), uint8)\n",
            "Action Space: Discrete(6)\n",
            "Observation Shape: (210, 160, 3)\n",
            "Number of Actions: 6\n",
            "\n",
            "Sample Observation Shape: (210, 160, 3)\n"
          ]
        }
      ],
      "source": [
        "def explore_environment(env_name):\n",
        "    \"\"\"Load environment and print state/action space information\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Environment: {env_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    print(f\"\\nObservation Space: {env.observation_space}\")\n",
        "    print(f\"Action Space: {env.action_space}\")\n",
        "\n",
        "    if isinstance(env.observation_space, gym.spaces.Box):\n",
        "        print(f\"Observation Shape: {env.observation_space.shape}\")\n",
        "        if len(env.observation_space.shape) == 1:\n",
        "            print(f\"Observation Low: {env.observation_space.low}\")\n",
        "            print(f\"Observation High: {env.observation_space.high}\")\n",
        "\n",
        "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "        print(f\"Number of Actions: {env.action_space.n}\")\n",
        "\n",
        "    # Sample observation\n",
        "    obs, _ = env.reset(seed=SEED)\n",
        "    print(f\"\\nSample Observation Shape: {np.array(obs).shape}\")\n",
        "    if len(np.array(obs).shape) == 1:\n",
        "        print(f\"Sample Observation: {obs}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "# Explore MountainCar\n",
        "explore_environment(\"MountainCar-v0\")\n",
        "\n",
        "# Explore Pong\n",
        "explore_environment(\"ALE/Pong-v5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVwfTX4_KbH3"
      },
      "source": [
        "### 2.2 Random Agent to Understand Reward Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XynlUHqmKbH3",
        "outputId": "fc5a50cc-bcb8-48ed-cc3f-a6212d17fdda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Random Agent Testing: MountainCar-v0\n",
            "============================================================\n",
            "Episode 1: Reward = -200.00, Steps = 200\n",
            "Episode 2: Reward = -200.00, Steps = 200\n",
            "Episode 3: Reward = -200.00, Steps = 200\n",
            "Episode 4: Reward = -200.00, Steps = 200\n",
            "Episode 5: Reward = -200.00, Steps = 200\n",
            "Episode 6: Reward = -200.00, Steps = 200\n",
            "Episode 7: Reward = -200.00, Steps = 200\n",
            "Episode 8: Reward = -200.00, Steps = 200\n",
            "Episode 9: Reward = -200.00, Steps = 200\n",
            "Episode 10: Reward = -200.00, Steps = 200\n",
            "\n",
            "Statistics:\n",
            "Mean Episode Reward: -200.00 ± 0.00\n",
            "Mean Episode Length: 200.00 ± 0.00\n",
            "Unique Rewards: [-1.]\n",
            "Reward Range: [-1.00, -1.00]\n",
            "\n",
            "============================================================\n",
            "Random Agent Testing: ALE/Pong-v5\n",
            "============================================================\n",
            "Episode 1: Reward = -21.00, Steps = 999\n",
            "Episode 2: Reward = -21.00, Steps = 886\n",
            "Episode 3: Reward = -21.00, Steps = 884\n",
            "Episode 4: Reward = -20.00, Steps = 903\n",
            "Episode 5: Reward = -21.00, Steps = 901\n",
            "\n",
            "Statistics:\n",
            "Mean Episode Reward: -20.80 ± 0.40\n",
            "Mean Episode Length: 914.60 ± 42.89\n",
            "Unique Rewards: [-1.  0.  1.]\n",
            "Reward Range: [-1.00, 1.00]\n"
          ]
        }
      ],
      "source": [
        "def test_random_agent(env_name, num_episodes=10, max_steps=1000):\n",
        "    \"\"\"Test random agent and analyze rewards\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Random Agent Testing: {env_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    all_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs, _ = env.reset(seed=SEED + episode)\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        done = False\n",
        "        truncated = False\n",
        "\n",
        "        while not (done or truncated) and steps < max_steps:\n",
        "            action = env.action_space.sample()  # Random action\n",
        "            obs, reward, done, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            all_rewards.append(reward)\n",
        "            steps += 1\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_lengths.append(steps)\n",
        "        print(f\"Episode {episode+1}: Reward = {total_reward:.2f}, Steps = {steps}\")\n",
        "\n",
        "    print(f\"\\nStatistics:\")\n",
        "    print(f\"Mean Episode Reward: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
        "    print(f\"Mean Episode Length: {np.mean(episode_lengths):.2f} ± {np.std(episode_lengths):.2f}\")\n",
        "    print(f\"Unique Rewards: {np.unique(all_rewards)}\")\n",
        "    print(f\"Reward Range: [{np.min(all_rewards):.2f}, {np.max(all_rewards):.2f}]\")\n",
        "\n",
        "    env.close()\n",
        "    return episode_rewards, episode_lengths\n",
        "\n",
        "# Test random agents\n",
        "mc_rewards, mc_lengths = test_random_agent(\"MountainCar-v0\", num_episodes=10)\n",
        "pong_rewards, pong_lengths = test_random_agent(\"ALE/Pong-v5\", num_episodes=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDlQVedAKbH4"
      },
      "source": [
        "### Observations from Random Agents\n",
        "\n",
        "**MountainCar-v0:**\n",
        "- **State space**: 2D continuous (position ∈ [-1.2, 0.6], velocity ∈ [-0.07, 0.07])\n",
        "- **Action space**: 3 discrete actions (0=push left, 1=no push, 2=push right)\n",
        "- **Reward**: -1 for each time step until goal is reached\n",
        "- **Random agent**: Typically gets rewards around -200 (episode length limit)\n",
        "- **Challenge**: Need to build momentum by going back and forth to reach the goal\n",
        "\n",
        "**Pong-v0:**\n",
        "- **State space**: RGB image (210×160×3)\n",
        "- **Action space**: 6 discrete actions (NOOP, FIRE, RIGHT, LEFT, RIGHTFIRE, LEFTFIRE)\n",
        "- **Reward**: +1 for scoring, -1 for opponent scoring, 0 otherwise\n",
        "- **Random agent**: Typically loses badly (around -21 to -15)\n",
        "- **Challenge**: High-dimensional visual input requires preprocessing and frame stacking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97miLMPcKbH4"
      },
      "source": [
        "## 3. Part B: DQN Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1_T2ni3KbH4"
      },
      "source": [
        "### 3.1 Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9dYVrQRRKbH4"
      },
      "outputs": [],
      "source": [
        "def train_dqn(env_name, agent, num_episodes=1000, max_steps=1000,\n",
        "              preprocessor=None, frame_stack=None, print_freq=10,\n",
        "              save_freq=100, save_path='dqn_checkpoint.pth'):\n",
        "    \"\"\"Train DQN agent\"\"\"\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    losses = []\n",
        "    mean_rewards = []\n",
        "    best_mean_reward = -float('inf')\n",
        "\n",
        "    total_steps = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "        # Preprocess initial observation\n",
        "        if preprocessor is not None:\n",
        "            preprocessor.reset()\n",
        "            processed_obs = preprocessor.preprocess(obs, use_diff=False)\n",
        "            if frame_stack is not None:\n",
        "                state = frame_stack.reset(processed_obs)\n",
        "            else:\n",
        "                state = processed_obs\n",
        "        else:\n",
        "            state = obs\n",
        "\n",
        "        episode_reward = 0\n",
        "        episode_loss = []\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            # Select action\n",
        "            action = agent.select_action(state, training=True)\n",
        "\n",
        "            # Take action\n",
        "            next_obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            # Preprocess next observation\n",
        "            if preprocessor is not None:\n",
        "                processed_next_obs = preprocessor.preprocess(next_obs, use_diff=False)\n",
        "                if frame_stack is not None:\n",
        "                    next_state = frame_stack.push(processed_next_obs)\n",
        "                else:\n",
        "                    next_state = processed_next_obs\n",
        "            else:\n",
        "                next_state = next_obs\n",
        "\n",
        "            # Store transition\n",
        "            agent.replay_buffer.push(state, action, next_state, reward, done or truncated)\n",
        "\n",
        "            # Update agent\n",
        "            loss = agent.update()\n",
        "            if loss is not None:\n",
        "                episode_loss.append(loss)\n",
        "\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            total_steps += 1\n",
        "\n",
        "            if done or truncated:\n",
        "                break\n",
        "\n",
        "        # Decay epsilon\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        # Record statistics\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_lengths.append(step + 1)\n",
        "        if episode_loss:\n",
        "            losses.append(np.mean(episode_loss))\n",
        "\n",
        "        # Calculate mean reward (last 100 episodes)\n",
        "        if len(episode_rewards) >= 100:\n",
        "            mean_reward = np.mean(episode_rewards[-100:])\n",
        "            mean_rewards.append(mean_reward)\n",
        "\n",
        "            if mean_reward > best_mean_reward:\n",
        "                best_mean_reward = mean_reward\n",
        "                agent.save(save_path.replace('.pth', '_best.pth'))\n",
        "\n",
        "        # Print progress\n",
        "        if (episode + 1) % print_freq == 0:\n",
        "            mean_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
        "            print(f\"Episode {episode+1}/{num_episodes} | \"\n",
        "                  f\"Steps: {total_steps} | \"\n",
        "                  f\"Reward: {episode_reward:.2f} | \"\n",
        "                  f\"Mean(100): {mean_reward:.2f} | \"\n",
        "                  f\"Epsilon: {agent.epsilon:.3f} | \"\n",
        "                  f\"Loss: {np.mean(episode_loss) if episode_loss else 0:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (episode + 1) % save_freq == 0:\n",
        "            agent.save(save_path)\n",
        "            print(f\"Checkpoint saved at episode {episode+1}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return {\n",
        "        'episode_rewards': episode_rewards,\n",
        "        'episode_lengths': episode_lengths,\n",
        "        'losses': losses,\n",
        "        'mean_rewards': mean_rewards,\n",
        "        'best_mean_reward': best_mean_reward\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c79h4cyiKbH5"
      },
      "source": [
        "### 3.2 Train DQN on MountainCar-v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Twm0EpNKbH5",
        "outputId": "9eab9e40-1971-4018-b2db-c077d2cc71da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training DQN on MountainCar-v0...\n",
            "This should take about 10-20 minutes on a modest laptop.\n",
            "Episode 10/500 | Steps: 2000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.951 | Loss: 1.0428\n",
            "Episode 20/500 | Steps: 4000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.905 | Loss: 2.8152\n",
            "Episode 30/500 | Steps: 6000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.860 | Loss: 6.4210\n",
            "Episode 40/500 | Steps: 8000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.818 | Loss: 10.0036\n",
            "Episode 50/500 | Steps: 10000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.778 | Loss: 13.3490\n",
            "Episode 60/500 | Steps: 12000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.740 | Loss: 15.1439\n",
            "Episode 70/500 | Steps: 14000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.704 | Loss: 17.9144\n",
            "Episode 80/500 | Steps: 16000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.670 | Loss: 18.4682\n",
            "Episode 90/500 | Steps: 18000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.637 | Loss: 17.4523\n",
            "Episode 100/500 | Steps: 20000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.606 | Loss: 19.3188\n",
            "Checkpoint saved at episode 100\n",
            "Episode 110/500 | Steps: 22000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.576 | Loss: 23.4093\n",
            "Episode 120/500 | Steps: 24000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.548 | Loss: 24.7891\n",
            "Episode 130/500 | Steps: 26000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.521 | Loss: 21.9073\n",
            "Episode 140/500 | Steps: 28000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.496 | Loss: 22.8623\n",
            "Episode 150/500 | Steps: 30000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.471 | Loss: 27.5239\n",
            "Episode 160/500 | Steps: 32000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.448 | Loss: 24.4731\n",
            "Episode 170/500 | Steps: 34000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.427 | Loss: 24.8872\n",
            "Episode 180/500 | Steps: 36000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.406 | Loss: 25.7996\n",
            "Episode 190/500 | Steps: 38000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.386 | Loss: 26.8916\n",
            "Episode 200/500 | Steps: 40000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.367 | Loss: 26.5399\n",
            "Checkpoint saved at episode 200\n",
            "Episode 210/500 | Steps: 42000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.349 | Loss: 28.6432\n",
            "Episode 220/500 | Steps: 44000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.332 | Loss: 23.7836\n",
            "Episode 230/500 | Steps: 46000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.316 | Loss: 26.8726\n",
            "Episode 240/500 | Steps: 48000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.300 | Loss: 23.2881\n",
            "Episode 250/500 | Steps: 50000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.286 | Loss: 24.7081\n",
            "Episode 260/500 | Steps: 52000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.272 | Loss: 22.0243\n",
            "Episode 270/500 | Steps: 54000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.258 | Loss: 30.3618\n",
            "Episode 280/500 | Steps: 56000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.246 | Loss: 20.0346\n",
            "Episode 290/500 | Steps: 58000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.234 | Loss: 25.4409\n",
            "Episode 300/500 | Steps: 60000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.222 | Loss: 29.7729\n",
            "Checkpoint saved at episode 300\n",
            "Episode 310/500 | Steps: 62000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.211 | Loss: 27.9393\n",
            "Episode 320/500 | Steps: 64000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.201 | Loss: 29.4834\n",
            "Episode 330/500 | Steps: 66000 | Reward: -200.00 | Mean(100): -200.00 | Epsilon: 0.191 | Loss: 28.6044\n",
            "Episode 340/500 | Steps: 67954 | Reward: -200.00 | Mean(100): -199.54 | Epsilon: 0.182 | Loss: 24.5517\n",
            "Episode 350/500 | Steps: 69810 | Reward: -133.00 | Mean(100): -198.10 | Epsilon: 0.173 | Loss: 23.9667\n",
            "Episode 360/500 | Steps: 71483 | Reward: -124.00 | Mean(100): -194.83 | Epsilon: 0.165 | Loss: 21.9821\n",
            "Episode 370/500 | Steps: 72938 | Reward: -156.00 | Mean(100): -189.38 | Epsilon: 0.157 | Loss: 20.7491\n",
            "Episode 380/500 | Steps: 74504 | Reward: -141.00 | Mean(100): -185.04 | Epsilon: 0.149 | Loss: 17.2377\n",
            "Episode 390/500 | Steps: 76054 | Reward: -144.00 | Mean(100): -180.54 | Epsilon: 0.142 | Loss: 17.8947\n",
            "Episode 400/500 | Steps: 77787 | Reward: -173.00 | Mean(100): -177.87 | Epsilon: 0.135 | Loss: 13.1529\n",
            "Checkpoint saved at episode 400\n",
            "Episode 410/500 | Steps: 79371 | Reward: -146.00 | Mean(100): -173.71 | Epsilon: 0.128 | Loss: 14.0033\n",
            "Episode 420/500 | Steps: 80718 | Reward: -120.00 | Mean(100): -167.18 | Epsilon: 0.122 | Loss: 11.6021\n",
            "Episode 430/500 | Steps: 82140 | Reward: -200.00 | Mean(100): -161.40 | Epsilon: 0.116 | Loss: 11.3003\n",
            "Episode 440/500 | Steps: 83517 | Reward: -118.00 | Mean(100): -155.63 | Epsilon: 0.110 | Loss: 9.8643\n",
            "Episode 450/500 | Steps: 84885 | Reward: -118.00 | Mean(100): -150.75 | Epsilon: 0.105 | Loss: 6.7944\n",
            "Episode 460/500 | Steps: 86275 | Reward: -115.00 | Mean(100): -147.92 | Epsilon: 0.100 | Loss: 7.4200\n",
            "Episode 470/500 | Steps: 87541 | Reward: -111.00 | Mean(100): -146.03 | Epsilon: 0.095 | Loss: 7.2379\n"
          ]
        }
      ],
      "source": [
        "# Create MountainCar agent\n",
        "mc_agent = DQNAgent(\n",
        "    state_dim=2,\n",
        "    action_dim=3,\n",
        "    network_type='mlp',\n",
        "    lr=1e-3,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.01,\n",
        "    epsilon_decay=0.995,\n",
        "    buffer_size=50000,\n",
        "    batch_size=64,\n",
        "    target_update_freq=100\n",
        ")\n",
        "\n",
        "print(\"Training DQN on MountainCar-v0...\")\n",
        "print(\"This should take about 10-20 minutes on a modest laptop.\")\n",
        "\n",
        "# Train\n",
        "mc_results = train_dqn(\n",
        "    env_name=\"MountainCar-v0\",\n",
        "    agent=mc_agent,\n",
        "    num_episodes=500,\n",
        "    max_steps=200,\n",
        "    print_freq=10,\n",
        "    save_freq=100,\n",
        "    save_path='mountaincar_dqn.pth'\n",
        ")\n",
        "\n",
        "# Save results\n",
        "save_results(mc_results, 'mountaincar_results.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGL4fDlCKbH5"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plot_training_curve(mc_results, 'MountainCar-v0', window=50, save_path='mountaincar_training.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y62RM_npKbH5"
      },
      "outputs": [],
      "source": [
        "# Plot policy visualization\n",
        "plot_mountaincar_policy(mc_agent, resolution=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeMZpBeTKbH5"
      },
      "source": [
        "### 3.3 Train DQN on Pong-v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqDvCS6jKbH5"
      },
      "outputs": [],
      "source": [
        "# Create Pong agent with CNN\n",
        "pong_agent = DQNAgent(\n",
        "    state_dim=4,  # 4 stacked frames\n",
        "    action_dim=6,\n",
        "    network_type='cnn',\n",
        "    lr=1e-4,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.02,\n",
        "    epsilon_decay=0.9999,\n",
        "    buffer_size=100000,\n",
        "    batch_size=32,\n",
        "    target_update_freq=1000\n",
        ")\n",
        "\n",
        "# Create preprocessor and frame stack\n",
        "pong_preprocessor = FramePreprocessor(frame_size=(84, 84))\n",
        "pong_frame_stack = FrameStack(num_frames=4)\n",
        "\n",
        "print(\"Training DQN on Pong-v0...\")\n",
        "print(\"This will take several hours. Consider reducing num_episodes for testing.\")\n",
        "print(\"For a reasonable Pong player, train for 2000-4000 episodes (2-4 million steps).\")\n",
        "\n",
        "# Train (reduce num_episodes for quick testing)\n",
        "pong_results = train_dqn(\n",
        "    env_name=\"ALE/Pong-v5\",\n",
        "    agent=pong_agent,\n",
        "    num_episodes=2000,  # Increase to 3000-4000 for better performance\n",
        "    max_steps=10000,\n",
        "    preprocessor=pong_preprocessor,\n",
        "    frame_stack=pong_frame_stack,\n",
        "    print_freq=10,\n",
        "    save_freq=100,\n",
        "    save_path='pong_dqn.pth'\n",
        ")\n",
        "\n",
        "# Save results\n",
        "save_results(pong_results, 'pong_results.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzO_Le0_KbH5"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plot_training_curve(pong_results, 'Pong-v0', window=100, save_path='pong_training.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRENBW4sKbH6"
      },
      "source": [
        "### 3.4 Test Trained Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wa2-3lhKbH6"
      },
      "outputs": [],
      "source": [
        "def test_agent(env_name, agent, num_episodes=10, preprocessor=None, frame_stack=None, render=False):\n",
        "    \"\"\"Test trained agent\"\"\"\n",
        "    if render:\n",
        "        env = gym.make(env_name, render_mode='human')\n",
        "    else:\n",
        "        env = gym.make(env_name)\n",
        "\n",
        "    test_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "        if preprocessor is not None:\n",
        "            preprocessor.reset()\n",
        "            processed_obs = preprocessor.preprocess(obs, use_diff=False)\n",
        "            if frame_stack is not None:\n",
        "                state = frame_stack.reset(processed_obs)\n",
        "            else:\n",
        "                state = processed_obs\n",
        "        else:\n",
        "            state = obs\n",
        "\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        truncated = False\n",
        "\n",
        "        while not (done or truncated):\n",
        "            action = agent.select_action(state, training=False)\n",
        "            next_obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            if preprocessor is not None:\n",
        "                processed_next_obs = preprocessor.preprocess(next_obs, use_diff=False)\n",
        "                if frame_stack is not None:\n",
        "                    next_state = frame_stack.push(processed_next_obs)\n",
        "                else:\n",
        "                    next_state = processed_next_obs\n",
        "            else:\n",
        "                next_state = next_obs\n",
        "\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        test_rewards.append(episode_reward)\n",
        "        print(f\"Test Episode {episode+1}: Reward = {episode_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(f\"\\nTest Results:\")\n",
        "    print(f\"Mean Reward: {np.mean(test_rewards):.2f} ± {np.std(test_rewards):.2f}\")\n",
        "    print(f\"Min Reward: {np.min(test_rewards):.2f}\")\n",
        "    print(f\"Max Reward: {np.max(test_rewards):.2f}\")\n",
        "\n",
        "    return test_rewards\n",
        "\n",
        "# Test MountainCar agent\n",
        "print(\"Testing MountainCar agent...\")\n",
        "mc_test_rewards = test_agent(\"MountainCar-v0\", mc_agent, num_episodes=10)\n",
        "\n",
        "# Test Pong agent\n",
        "print(\"\\nTesting Pong agent...\")\n",
        "pong_test_rewards = test_agent(\"ALE/Pong-v5\", pong_agent, num_episodes=5,\n",
        "                                preprocessor=pong_preprocessor, frame_stack=pong_frame_stack)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ifr-ti6KbH6"
      },
      "source": [
        "## 4. Summary and Analysis\n",
        "\n",
        "### MountainCar-v0 Results:\n",
        "- The DQN agent should learn to solve MountainCar within 300-500 episodes\n",
        "- Successful agents achieve rewards around -110 to -90 (reaching goal in 90-110 steps)\n",
        "- The policy visualization shows the agent learns to:\n",
        "  - Push right when moving right with positive velocity\n",
        "  - Push left when moving left to build momentum\n",
        "  - Use the valley strategically to gain speed\n",
        "\n",
        "### Pong-v0 Results:\n",
        "- Training requires 2000-4000 episodes (2-4 million steps) for reasonable performance\n",
        "- Random agent scores around -21 (loses every point)\n",
        "- After training:\n",
        "  - Early training (500-1000 episodes): Agent starts to track the ball\n",
        "  - Mid training (1000-2000 episodes): Agent wins occasional points (score -15 to -10)\n",
        "  - Late training (2000+ episodes): Agent becomes competitive (score -5 to +5)\n",
        "- Frame preprocessing (grayscale + downsampling) reduces computation significantly\n",
        "- Frame stacking (4 frames) helps capture motion information\n",
        "\n",
        "### Key Observations:\n",
        "1. **Experience Replay**: Critical for stable learning, breaks correlation between consecutive samples\n",
        "2. **Target Network**: Prevents moving target problem, updated every 1000 steps\n",
        "3. **Epsilon Decay**: Balances exploration vs exploitation\n",
        "4. **Preprocessing**: Essential for Atari games to reduce input dimensionality\n",
        "5. **Hyperparameters**: Learning rate, batch size, and network architecture significantly impact performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF98SDuzKbH6"
      },
      "source": [
        "## 5. Load and Continue Training (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-1mIJbjKbH6"
      },
      "outputs": [],
      "source": [
        "# Load saved checkpoint and continue training\n",
        "# mc_agent.load('mountaincar_dqn_best.pth')\n",
        "# pong_agent.load('pong_dqn_best.pth')\n",
        "\n",
        "# Continue training with more episodes\n",
        "# mc_results_continued = train_dqn(...)\n",
        "# pong_results_continued = train_dqn(...)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}