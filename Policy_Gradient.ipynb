{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient with Variance Reduction Techniques\n",
    "\n",
    "This notebook implements the REINFORCE policy gradient algorithm with variance reduction techniques:\n",
    "- **Reward-to-go**: Computing returns from current timestep onwards\n",
    "- **Advantage normalization**: Normalizing advantages to have mean 0 and std 1\n",
    "- **Baseline function**: Using value function to reduce variance\n",
    "\n",
    "## Environments:\n",
    "- **CartPole-v1**: Classic control problem\n",
    "- **LunarLander-v2**: Continuous control with discrete actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install gymnasium torch matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Import custom modules\n",
    "from pg_agent import PolicyGradientAgent, device\n",
    "from pg_utils import (\n",
    "    plot_learning_curves, \n",
    "    plot_single_training_curve,\n",
    "    compare_configurations,\n",
    "    save_results, \n",
    "    load_results\n",
    ")\n",
    "from train_pg import train_policy_gradient\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part 1: Environment Exploration\n",
    "\n",
    "Load environments and understand their characteristics using random agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Environments and Inspect State/Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_environment(env_name):\n",
    "    \"\"\"Load environment and print state/action space information\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Environment: {env_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    print(f\"\\nObservation Space: {env.observation_space}\")\n",
    "    print(f\"Action Space: {env.action_space}\")\n",
    "    \n",
    "    if isinstance(env.observation_space, gym.spaces.Box):\n",
    "        print(f\"Observation Shape: {env.observation_space.shape}\")\n",
    "        print(f\"Observation Low: {env.observation_space.low}\")\n",
    "        print(f\"Observation High: {env.observation_space.high}\")\n",
    "    \n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        print(f\"Number of Actions: {env.action_space.n}\")\n",
    "    \n",
    "    # Sample observation\n",
    "    obs, _ = env.reset(seed=SEED)\n",
    "    print(f\"\\nSample Observation: {obs}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# Explore CartPole\n",
    "explore_environment(\"CartPole-v1\")\n",
    "\n",
    "# Explore LunarLander\n",
    "explore_environment(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Agent to Understand Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_agent(env_name, num_episodes=10, max_steps=1000):\n",
    "    \"\"\"Test random agent and analyze rewards\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Random Agent Testing: {env_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset(seed=SEED + episode)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (done or truncated) and steps < max_steps:\n",
    "            action = env.action_space.sample()  # Random action\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            all_rewards.append(reward)\n",
    "            steps += 1\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        print(f\"Episode {episode+1}: Reward = {total_reward:.2f}, Steps = {steps}\")\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Mean Episode Reward: {np.mean(episode_rewards):.2f} \u00b1 {np.std(episode_rewards):.2f}\")\n",
    "    print(f\"Mean Episode Length: {np.mean(episode_lengths):.2f} \u00b1 {np.std(episode_lengths):.2f}\")\n",
    "    print(f\"Reward Range: [{np.min(all_rewards):.2f}, {np.max(all_rewards):.2f}]\")\n",
    "    \n",
    "    env.close()\n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "# Test random agents\n",
    "cartpole_rewards, cartpole_lengths = test_random_agent(\"CartPole-v1\", num_episodes=10)\n",
    "lunar_rewards, lunar_lengths = test_random_agent(\"LunarLander-v2\", num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Observations from Random Agents\n",
    "\n",
    "**CartPole-v1:**\n",
    "- **State space**: 4D continuous (cart position, cart velocity, pole angle, pole angular velocity)\n",
    "- **Action space**: 2 discrete actions (push left=0, push right=1)\n",
    "- **Reward**: +1 for every timestep the pole stays upright\n",
    "- **Random agent**: Typically achieves 15-30 steps before failure\n",
    "- **Challenge**: Need to learn coordinated actions to balance pole\n",
    "- **Success criterion**: Average reward of 475+ over 100 episodes\n",
    "\n",
    "**LunarLander-v2:**\n",
    "- **State space**: 8D continuous (position, velocity, angle, angular velocity, leg contact)\n",
    "- **Action space**: 4 discrete actions (do nothing, fire left, fire main, fire right)\n",
    "- **Reward**: \n",
    "  - Positive for moving towards landing pad\n",
    "  - Large positive for landing successfully\n",
    "  - Negative for crashing or using fuel\n",
    "- **Random agent**: Typically achieves -200 to -100 (crashes)\n",
    "- **Challenge**: Complex dynamics requiring careful control\n",
    "- **Success criterion**: Average reward of 200+ over 100 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Part 2: Policy Gradient Implementation\n",
    "\n",
    "Implement and compare different variance reduction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training Configurations\n",
    "\n",
    "We will test 4 different configurations:\n",
    "1. **Baseline**: No reward-to-go, no advantage normalization\n",
    "2. **Reward-to-go**: With reward-to-go, no advantage normalization\n",
    "3. **Advantage normalization**: With both reward-to-go and advantage normalization\n",
    "4. **Full (with baseline)**: All variance reduction techniques including value baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train on CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters for CartPole\n",
    "cartpole_params = {\n",
    "    'env_name': 'CartPole-v1',\n",
    "    'num_iterations': 100,\n",
    "    'batch_size': 5000,\n",
    "    'lr': 1e-2,\n",
    "    'gamma': 0.99,\n",
    "    'hidden_sizes': [32, 32],\n",
    "    'max_episode_length': 500,\n",
    "    'print_freq': 10,\n",
    "    'seed': SEED\n",
    "}\n",
    "\n",
    "print(\"Training Policy Gradient on CartPole-v1 with different configurations...\")\n",
    "print(\"This will take approximately 15-20 minutes.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 1: Baseline (no variance reduction)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Configuration 1: Baseline (No Variance Reduction)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cartpole_baseline = train_policy_gradient(\n",
    "    **cartpole_params,\n",
    "    use_reward_to_go=False,\n",
    "    use_advantage_normalization=False,\n",
    "    use_baseline=False,\n",
    "    save_path='pg_cartpole_baseline.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 2: Reward-to-go only\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Configuration 2: Reward-to-go\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cartpole_rtg = train_policy_gradient(\n",
    "    **cartpole_params,\n",
    "    use_reward_to_go=True,\n",
    "    use_advantage_normalization=False,\n",
    "    use_baseline=False,\n",
    "    save_path='pg_cartpole_rtg.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 3: Reward-to-go + Advantage normalization\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Configuration 3: Reward-to-go + Advantage Normalization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cartpole_rtg_norm = train_policy_gradient(\n",
    "    **cartpole_params,\n",
    "    use_reward_to_go=True,\n",
    "    use_advantage_normalization=True,\n",
    "    use_baseline=False,\n",
    "    save_path='pg_cartpole_rtg_norm.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 4: Full (all variance reduction techniques)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Configuration 4: Full (RTG + Norm + Baseline)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cartpole_full = train_policy_gradient(\n",
    "    **cartpole_params,\n",
    "    use_reward_to_go=True,\n",
    "    use_advantage_normalization=True,\n",
    "    use_baseline=True,\n",
    "    save_path='pg_cartpole_full.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all CartPole configurations\n",
    "cartpole_results = {\n",
    "    'Baseline': cartpole_baseline,\n",
    "    'Reward-to-go': cartpole_rtg,\n",
    "    'RTG + Norm': cartpole_rtg_norm,\n",
    "    'Full (RTG + Norm + Baseline)': cartpole_full\n",
    "}\n",
    "\n",
    "plot_learning_curves(cartpole_results, 'CartPole-v1', window=5, \n",
    "                    save_path='cartpole_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "compare_configurations(cartpole_results, 'CartPole-v1', \n",
    "                      save_path='cartpole_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train on LunarLander-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters for LunarLander\n",
    "lunar_params = {\n",
    "    'env_name': 'LunarLander-v2',\n",
    "    'num_iterations': 200,\n",
    "    'batch_size': 5000,\n",
    "    'lr': 5e-3,\n",
    "    'gamma': 0.99,\n",
    "    'hidden_sizes': [64, 64],\n",
    "    'max_episode_length': 1000,\n",
    "    'print_freq': 10,\n",
    "    'seed': SEED\n",
    "}\n",
    "\n",
    "print(\"Training Policy Gradient on LunarLander-v2 with different configurations...\")\n",
    "print(\"This will take approximately 30-40 minutes.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 1: Baseline (no variance reduction)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Configuration 1: Baseline (No Variance Reduction)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lunar_baseline = train_policy_gradient(\n",
    "    **lunar_params,\n",
    "    use_reward_to_go=False,\n",
    "    use_advantage_normalization=False,\n",
    "    use_baseline=False,\n",
    "    save_path='pg_lunar_baseline.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 2: Reward-to-go only\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Configuration 2: Reward-to-go\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lunar_rtg = train_policy_gradient(\n",
    "    **lunar_params,\n",
    "    use_reward_to_go=True,\n",
    "    use_advantage_normalization=False,\n",
    "    use_baseline=False,\n",
    "    save_path='pg_lunar_rtg.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 3: Reward-to-go + Advantage normalization\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Configuration 3: Reward-to-go + Advantage Normalization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lunar_rtg_norm = train_policy_gradient(\n",
    "    **lunar_params,\n",
    "    use_reward_to_go=True,\n",
    "    use_advantage_normalization=True,\n",
    "    use_baseline=False,\n",
    "    save_path='pg_lunar_rtg_norm.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 4: Full (all variance reduction techniques)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Configuration 4: Full (RTG + Norm + Baseline)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lunar_full = train_policy_gradient(\n",
    "    **lunar_params,\n",
    "    use_reward_to_go=True,\n",
    "    use_advantage_normalization=True,\n",
    "    use_baseline=True,\n",
    "    save_path='pg_lunar_full.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all LunarLander configurations\n",
    "lunar_results = {\n",
    "    'Baseline': lunar_baseline,\n",
    "    'Reward-to-go': lunar_rtg,\n",
    "    'RTG + Norm': lunar_rtg_norm,\n",
    "    'Full (RTG + Norm + Baseline)': lunar_full\n",
    "}\n",
    "\n",
    "plot_learning_curves(lunar_results, 'LunarLander-v2', window=10, \n",
    "                    save_path='lunar_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "compare_configurations(lunar_results, 'LunarLander-v2', \n",
    "                      save_path='lunar_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Part 3: Impact of Batch Size on Policy Gradient\n",
    "\n",
    "In this section, we study how batch size affects policy gradient estimates. Batch size determines how many timesteps we collect before performing a policy update.\n",
    "\n",
    "**Key Questions:**\n",
    "1. How does batch size affect variance of gradient estimates?\n",
    "2. How does batch size impact convergence speed?\n",
    "3. What is the trade-off between sample efficiency and computational cost?\n",
    "4. What is the optimal batch size for each environment?\n",
    "\n",
    "**Batch Sizes to Test:**\n",
    "- **1000**: Small batch (high variance, fast updates)\n",
    "- **2500**: Medium-small batch\n",
    "- **5000**: Medium batch (baseline)\n",
    "- **10000**: Large batch (low variance, slow updates)\n",
    "\n",
    "We will use the **full configuration** (RTG + Norm + Baseline) for all experiments to isolate the effect of batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Batch Size Experiments on CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch sizes to test\n",
    "batch_sizes = [1000, 2500, 5000, 10000]\n",
    "cartpole_batch_results = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BATCH SIZE STUDY: CartPole-v1\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTesting {len(batch_sizes)} different batch sizes\")\n",
    "print(f\"Batch sizes: {batch_sizes}\")\n",
    "print(f\"Using full variance reduction (RTG + Norm + Baseline)\")\n",
    "print(f\"This will take approximately {len(batch_sizes) * 4} minutes\\n\")\n",
    "\n",
    "# Base parameters for CartPole\n",
    "base_params = {\n",
    "    'env_name': 'CartPole-v1',\n",
    "    'num_iterations': 100,\n",
    "    'lr': 1e-2,\n",
    "    'gamma': 0.99,\n",
    "    'use_reward_to_go': True,\n",
    "    'use_advantage_normalization': True,\n",
    "    'use_baseline': True,\n",
    "    'hidden_sizes': [32, 32],\n",
    "    'max_episode_length': 500,\n",
    "    'print_freq': 20,\n",
    "    'seed': SEED\n",
    "}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training with Batch Size: {batch_size}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = train_policy_gradient(\n",
    "        **base_params,\n",
    "        batch_size=batch_size,\n",
    "        save_path=f'pg_cartpole_batch_{batch_size}.pkl'\n",
    "    )\n",
    "    \n",
    "    cartpole_batch_results[f'Batch {batch_size}'] = results\n",
    "    \n",
    "    print(f\"\\nCompleted batch size {batch_size}\")\n",
    "    print(f\"Final mean return: {results['iteration_returns'][-1]:.2f}\")\n",
    "    print(f\"Best mean return: {max(results['iteration_returns']):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CartPole batch size experiments completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for different batch sizes\n",
    "plot_learning_curves(cartpole_batch_results, 'CartPole-v1 - Batch Size Comparison', \n",
    "                    window=5, save_path='cartpole_batch_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "compare_configurations(cartpole_batch_results, 'CartPole-v1 - Batch Size Impact',\n",
    "                      save_path='cartpole_batch_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Batch Size Experiments on LunarLander-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch sizes to test (same as CartPole)\n",
    "lunar_batch_results = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BATCH SIZE STUDY: LunarLander-v2\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTesting {len(batch_sizes)} different batch sizes\")\n",
    "print(f\"Batch sizes: {batch_sizes}\")\n",
    "print(f\"Using full variance reduction (RTG + Norm + Baseline)\")\n",
    "print(f\"This will take approximately {len(batch_sizes) * 10} minutes\\n\")\n",
    "\n",
    "# Base parameters for LunarLander\n",
    "base_params = {\n",
    "    'env_name': 'LunarLander-v2',\n",
    "    'num_iterations': 200,\n",
    "    'lr': 5e-3,\n",
    "    'gamma': 0.99,\n",
    "    'use_reward_to_go': True,\n",
    "    'use_advantage_normalization': True,\n",
    "    'use_baseline': True,\n",
    "    'hidden_sizes': [64, 64],\n",
    "    'max_episode_length': 1000,\n",
    "    'print_freq': 20,\n",
    "    'seed': SEED\n",
    "}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training with Batch Size: {batch_size}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = train_policy_gradient(\n",
    "        **base_params,\n",
    "        batch_size=batch_size,\n",
    "        save_path=f'pg_lunar_batch_{batch_size}.pkl'\n",
    "    )\n",
    "    \n",
    "    lunar_batch_results[f'Batch {batch_size}'] = results\n",
    "    \n",
    "    print(f\"\\nCompleted batch size {batch_size}\")\n",
    "    print(f\"Final mean return: {results['iteration_returns'][-1]:.2f}\")\n",
    "    print(f\"Best mean return: {max(results['iteration_returns']):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LunarLander batch size experiments completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for different batch sizes\n",
    "plot_learning_curves(lunar_batch_results, 'LunarLander-v2 - Batch Size Comparison',\n",
    "                    window=10, save_path='lunar_batch_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "compare_configurations(lunar_batch_results, 'LunarLander-v2 - Batch Size Impact',\n",
    "                      save_path='lunar_batch_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Batch Size Analysis and Observations\n",
    "\n",
    "#### Impact of Batch Size on Policy Gradient Estimates\n",
    "\n",
    "**Theoretical Background:**\n",
    "\n",
    "Batch size affects the variance and bias of policy gradient estimates:\n",
    "- **Larger batches**: Lower variance (more samples to average), but slower updates\n",
    "- **Smaller batches**: Higher variance (fewer samples), but faster updates\n",
    "\n",
    "The policy gradient estimator is:\n",
    "```\n",
    "\u2207J(\u03b8) \u2248 (1/N) \u2211_{i=1}^N \u2211_t \u2207log \u03c0_\u03b8(a_t|s_t) * A_t\n",
    "```\n",
    "\n",
    "Where N is the number of trajectories in the batch. By the Central Limit Theorem, variance decreases as O(1/\u221aN).\n",
    "\n",
    "#### Expected Observations:\n",
    "\n",
    "**Small Batch Size (1000):**\n",
    "- **Pros**: Fast updates, more iterations per unit time\n",
    "- **Cons**: High variance, noisy gradients, unstable learning\n",
    "- **Expected behavior**: Oscillating learning curves, may converge slowly\n",
    "\n",
    "**Medium-Small Batch Size (2500):**\n",
    "- **Pros**: Good balance, reasonable variance\n",
    "- **Cons**: Still some noise in gradients\n",
    "- **Expected behavior**: Moderate stability, decent convergence\n",
    "\n",
    "**Medium Batch Size (5000):**\n",
    "- **Pros**: Low variance, stable learning, good sample efficiency\n",
    "- **Cons**: Slower updates than smaller batches\n",
    "- **Expected behavior**: Smooth learning curves, reliable convergence\n",
    "\n",
    "**Large Batch Size (10000):**\n",
    "- **Pros**: Very low variance, very stable gradients\n",
    "- **Cons**: Slow updates, may be sample inefficient\n",
    "- **Expected behavior**: Very smooth curves, but may converge slowly in wall-clock time\n",
    "\n",
    "#### Key Insights:\n",
    "\n",
    "1. **Variance-Speed Tradeoff**: Larger batches reduce gradient variance but require more samples per update\n",
    "2. **Sample Efficiency**: Medium batch sizes (2500-5000) often provide best sample efficiency\n",
    "3. **Computational Efficiency**: Smaller batches allow more frequent updates\n",
    "4. **Environment Dependency**: Optimal batch size depends on environment complexity\n",
    "5. **Diminishing Returns**: Beyond a certain point, increasing batch size provides minimal benefit\n",
    "\n",
    "#### Recommendations:\n",
    "\n",
    "**For CartPole-v1:**\n",
    "- Optimal batch size: 2500-5000\n",
    "- Simpler environment, smaller batches work well\n",
    "- High variance is tolerable due to fast episodes\n",
    "\n",
    "**For LunarLander-v2:**\n",
    "- Optimal batch size: 5000-10000\n",
    "- More complex environment benefits from larger batches\n",
    "- Lower variance crucial for stable learning\n",
    "\n",
    "**General Guidelines:**\n",
    "1. Start with batch size of 5000 timesteps\n",
    "2. If learning is unstable, increase batch size\n",
    "3. If learning is too slow, decrease batch size\n",
    "4. Monitor both sample efficiency and wall-clock time\n",
    "5. Consider computational resources when choosing batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed quantitative comparison\n",
    "print(\"=\"*80)\n",
    "print(\"BATCH SIZE IMPACT - QUANTITATIVE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CartPole-v1 Results:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Batch Size':<15} {'Final Return':<20} {'Best Return':<20} {'Std Dev (last 20)':<20}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for config_name, results in cartpole_batch_results.items():\n",
    "    returns = results['iteration_returns']\n",
    "    final_return = np.mean(returns[-10:])\n",
    "    best_return = np.max(returns)\n",
    "    std_dev = np.std(returns[-20:])\n",
    "    print(f\"{config_name:<15} {final_return:<20.2f} {best_return:<20.2f} {std_dev:<20.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LunarLander-v2 Results:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Batch Size':<15} {'Final Return':<20} {'Best Return':<20} {'Std Dev (last 20)':<20}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for config_name, results in lunar_batch_results.items():\n",
    "    returns = results['iteration_returns']\n",
    "    final_return = np.mean(returns[-10:])\n",
    "    best_return = np.max(returns)\n",
    "    std_dev = np.std(returns[-20:])\n",
    "    print(f\"{config_name:<15} {final_return:<20.2f} {best_return:<20.2f} {std_dev:<20.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Key Observations:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Variance (Std Dev): Should decrease with larger batch sizes\")\n",
    "print(\"2. Final Return: Should be similar across batch sizes (if converged)\")\n",
    "print(\"3. Best Return: Indicates peak performance achieved\")\n",
    "print(\"4. Stability: Lower std dev indicates more stable learning\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variance vs batch size\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# CartPole variance analysis\n",
    "ax = axes[0]\n",
    "batch_sizes_list = [1000, 2500, 5000, 10000]\n",
    "cartpole_variances = []\n",
    "\n",
    "for batch_size in batch_sizes_list:\n",
    "    config_name = f'Batch {batch_size}'\n",
    "    if config_name in cartpole_batch_results:\n",
    "        returns = cartpole_batch_results[config_name]['iteration_returns']\n",
    "        variance = np.std(returns[-20:])  # Variance in last 20 iterations\n",
    "        cartpole_variances.append(variance)\n",
    "\n",
    "ax.plot(batch_sizes_list, cartpole_variances, marker='o', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Batch Size', fontsize=12)\n",
    "ax.set_ylabel('Standard Deviation (last 20 iter)', fontsize=12)\n",
    "ax.set_title('CartPole-v1: Variance vs Batch Size', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# LunarLander variance analysis\n",
    "ax = axes[1]\n",
    "lunar_variances = []\n",
    "\n",
    "for batch_size in batch_sizes_list:\n",
    "    config_name = f'Batch {batch_size}'\n",
    "    if config_name in lunar_batch_results:\n",
    "        returns = lunar_batch_results[config_name]['iteration_returns']\n",
    "        variance = np.std(returns[-20:])\n",
    "        lunar_variances.append(variance)\n",
    "\n",
    "ax.plot(batch_sizes_list, lunar_variances, marker='o', linewidth=2, markersize=8, color='orange')\n",
    "ax.set_xlabel('Batch Size', fontsize=12)\n",
    "ax.set_ylabel('Standard Deviation (last 20 iter)', fontsize=12)\n",
    "ax.set_title('LunarLander-v2: Variance vs Batch Size', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('batch_size_variance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Variance analysis plot saved to batch_size_variance_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nExpected Pattern: Variance should decrease as batch size increases\")\n",
    "print(\"This confirms the theoretical prediction: Var \u221d 1/\u221aN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Analysis and Observations\n\n### 4.1 CartPole-v1 Results\n\n**Expected Observations:**\n- **Baseline**: High variance, slower convergence, unstable learning\n- **Reward-to-go**: Reduced variance, faster convergence than baseline\n- **RTG + Normalization**: More stable learning, consistent improvement\n- **Full (with baseline)**: Best performance, fastest convergence, most stable\n\n**Key Insights:**\n1. Reward-to-go significantly reduces variance by only considering future rewards\n2. Advantage normalization stabilizes training by centering advantages\n3. Value baseline further reduces variance by subtracting state-dependent baseline\n4. Combined techniques provide best results\n\n### 4.2 LunarLander-v2 Results\n\n**Expected Observations:**\n- More challenging environment shows greater benefit from variance reduction\n- Baseline configuration may fail to learn or learn very slowly\n- Full configuration should achieve positive rewards consistently\n- Advantage normalization is crucial for this environment\n\n**Key Insights:**\n1. Variance reduction is essential for complex environments\n2. Without proper techniques, policy gradient can be very unstable\n3. Value baseline helps agent understand state-dependent expected returns\n4. Normalization prevents gradient explosion/vanishing\n\n### 4.3 Comparison of Variance Reduction Techniques\n\n**Reward-to-go (\u03a8t = Gt:\u221e):**\n- **Pros**: Reduces variance by ignoring past rewards, unbiased estimator\n- **Cons**: Still has high variance without normalization\n- **Impact**: Moderate improvement in convergence speed\n\n**Advantage Normalization:**\n- **Pros**: Stabilizes gradients, prevents extreme updates, improves sample efficiency\n- **Cons**: Adds computational overhead (minimal)\n- **Impact**: Significant improvement in stability\n\n**Value Baseline:**\n- **Pros**: State-dependent variance reduction, learns optimal baseline automatically\n- **Cons**: Requires additional network and training\n- **Impact**: Best variance reduction, fastest convergence\n\n### 4.4 Recommendations\n\n1. **Always use reward-to-go**: Minimal overhead, significant benefit\n2. **Use advantage normalization**: Essential for stable training\n3. **Use value baseline for complex tasks**: Worth the additional computation\n4. **Tune learning rate carefully**: Too high causes instability, too low is slow\n5. **Use appropriate batch size**: Larger batches reduce variance but increase computation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Command-Line Usage\n\nThe implementation can also be run from command line using `train_pg.py`:\n\n```bash\n# Basic usage with default settings\npython train_pg.py --env CartPole-v1\n\n# With custom parameters\npython train_pg.py --env LunarLander-v2 --num_iterations 200 --batch_size 5000 --lr 0.005\n\n# Without reward-to-go\npython train_pg.py --env CartPole-v1 --no_reward_to_go\n\n# Without advantage normalization\npython train_pg.py --env CartPole-v1 --no_advantage_norm\n\n# Without baseline\npython train_pg.py --env CartPole-v1 --no_baseline\n\n# All options disabled (baseline configuration)\npython train_pg.py --env CartPole-v1 --no_reward_to_go --no_advantage_norm --no_baseline\n\n# Custom network architecture\npython train_pg.py --env LunarLander-v2 --hidden_sizes 128 128 64\n\n# Save to specific path\npython train_pg.py --env CartPole-v1 --save_path my_results.pkl\n```\n\n### Available Command-Line Arguments:\n- `--env`: Environment name (default: CartPole-v1)\n- `--num_iterations`: Number of training iterations (default: 100)\n- `--batch_size`: Batch size in timesteps (default: 5000)\n- `--lr`: Learning rate (default: 3e-4)\n- `--gamma`: Discount factor (default: 0.99)\n- `--reward_to_go` / `--no_reward_to_go`: Enable/disable reward-to-go\n- `--advantage_norm` / `--no_advantage_norm`: Enable/disable advantage normalization\n- `--baseline` / `--no_baseline`: Enable/disable value baseline\n- `--hidden_sizes`: Network hidden layer sizes (default: 64 64)\n- `--max_episode_length`: Maximum episode length (default: 1000)\n- `--print_freq`: Print frequency (default: 10)\n- `--seed`: Random seed (default: 42)\n- `--save_path`: Path to save results (default: auto-generated)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Summary\n\nThis notebook demonstrated:\n1. \u2705 Environment exploration and random agent baselines\n2. \u2705 Policy gradient implementation with variance reduction techniques\n3. \u2705 Comparison of different configurations\n4. \u2705 Command-line interface for flexible training\n\n**Key Takeaways:**\n- Variance reduction is crucial for policy gradient methods\n- Reward-to-go provides unbiased variance reduction\n- Advantage normalization stabilizes training\n- Value baseline offers state-dependent variance reduction\n- Combined techniques provide best performance"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}